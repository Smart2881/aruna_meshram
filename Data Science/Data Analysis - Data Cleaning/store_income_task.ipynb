{"cells":[{"cell_type":"markdown","metadata":{"id":"lqt_yzRy16Wj"},"source":["## Task\n","\n","In this compulsory task you will clean the country column and parse the date column in the **store_income_data_task.csv** file."]},{"cell_type":"code","execution_count":59,"metadata":{"id":"vBP3WN2O16Wp"},"outputs":[],"source":["# Load up store_income_data.csv\n","import pandas as pd\n","import fuzzywuzzy\n","from fuzzywuzzy import process\n","import datetime\n","import chardet\n","\n","# Load up CSV file\n","income_df = pd.read_csv(\"store_income_data_task.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"ItqLwumA16Wr"},"source":["1. Take a look at all the unique values in the \"country\" column. Then, convert the column to lowercase and remove any trailing white spaces."]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Cullen/Frost Bankers, Inc.</td>\n","      <td>NaN</td>\n","      <td>Clothing</td>\n","      <td>$54438554.24</td>\n","      <td>4-2-2006</td>\n","      <td>united kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Nordson Corporation</td>\n","      <td>NaN</td>\n","      <td>Tools</td>\n","      <td>$41744177.01</td>\n","      <td>4-1-2006</td>\n","      <td>united kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Stag Industrial, Inc.</td>\n","      <td>NaN</td>\n","      <td>Beauty</td>\n","      <td>$36152340.34</td>\n","      <td>12-9-2003</td>\n","      <td>united kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>8-5-2006</td>\n","      <td>united kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Mercantile Bank Corporation</td>\n","      <td>NaN</td>\n","      <td>Baby</td>\n","      <td>$33552742.32</td>\n","      <td>21-1-1973</td>\n","      <td>united kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>Auburn National Bancorporation, Inc.</td>\n","      <td>ccaldeyroux5@dion.ne.jp</td>\n","      <td>Grocery</td>\n","      <td>$69798987.04</td>\n","      <td>19-9-1999</td>\n","      <td>u.k.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Interlink Electronics, Inc.</td>\n","      <td>orodenborch6@skyrock.com</td>\n","      <td>Garden</td>\n","      <td>$22521052.79</td>\n","      <td>8-6-2001</td>\n","      <td>sa</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>Tallgrass Energy Partners, LP</td>\n","      <td>NaN</td>\n","      <td>Grocery</td>\n","      <td>$54405380.40</td>\n","      <td>16-9-1992</td>\n","      <td>u.k/</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>Tronox Limited</td>\n","      <td>NaN</td>\n","      <td>Outdoors</td>\n","      <td>$99290004.13</td>\n","      <td>11-1-1992</td>\n","      <td>united kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>Synopsys, Inc.</td>\n","      <td>lcancellieri9@tmall.com</td>\n","      <td>Electronics</td>\n","      <td>$44091294.62</td>\n","      <td>11-7-2006</td>\n","      <td>united kingdom</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                            store_name               store_email  \\\n","0   1            Cullen/Frost Bankers, Inc.                       NaN   \n","1   2                   Nordson Corporation                       NaN   \n","2   3                 Stag Industrial, Inc.                       NaN   \n","3   4                   FIRST REPUBLIC BANK        ecanadine3@fc2.com   \n","4   5           Mercantile Bank Corporation                       NaN   \n","5   6  Auburn National Bancorporation, Inc.   ccaldeyroux5@dion.ne.jp   \n","6   7           Interlink Electronics, Inc.  orodenborch6@skyrock.com   \n","7   8         Tallgrass Energy Partners, LP                       NaN   \n","8   9                        Tronox Limited                       NaN   \n","9  10                        Synopsys, Inc.   lcancellieri9@tmall.com   \n","\n","    department        income date_measured         country  \n","0     Clothing  $54438554.24      4-2-2006  united kingdom  \n","1        Tools  $41744177.01      4-1-2006  united kingdom  \n","2       Beauty  $36152340.34     12-9-2003  united kingdom  \n","3   Automotive   $8928350.04      8-5-2006  united kingdom  \n","4         Baby  $33552742.32     21-1-1973  united kingdom  \n","5      Grocery  $69798987.04     19-9-1999            u.k.  \n","6       Garden  $22521052.79      8-6-2001              sa  \n","7      Grocery  $54405380.40     16-9-1992            u.k/  \n","8     Outdoors  $99290004.13     11-1-1992  united kingdom  \n","9  Electronics  $44091294.62     11-7-2006  united kingdom  "]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["income_df.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["1. Take a look at all the unique values in the \"country\" column. Then, convert the column to lowercase and remove any trailing white spaces."]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique countries before cleaning: 14\n","Number of unique countries after cleaning: 14\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_30076\\3732283081.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  income_df.country.fillna('unknown', inplace=True)\n"]},{"data":{"text/plain":["array(['united kingdom', 'u.k.', 'sa', 'u.k/', 'unknown', 'south africa',\n","       's.a', 'u.k', 'united states', 'sa.', 'sa/',\n","       's. africasouth africa', 's. africasouth africa/',\n","       's. africasouth africa.'], dtype=object)"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["# Display unique country names before cleaning\n","countries = income_df['country'].unique()\n","print(f\"Number of unique countries before cleaning: {len(countries)}\")\n","countries\n","\n","# Convert country names to lowercase and remove leading/trailing whitespaces\n","income_df['country'] = income_df['country'].str.lower().str.strip()\n","\n","# Replace all null/NaN value country names with 'unknown'\n","income_df.country.fillna('unknown', inplace=True)\n","\n","# Display unique country names after cleaning\n","countries = income_df['country'].unique()\n","print(f\"Number of unique countries after cleaning: {len(countries)}\")\n","countries"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"text/plain":["[('s.a', 100),\n"," ('sa', 40),\n"," ('south africa', 40),\n"," ('sa.', 40),\n"," ('sa/', 40),\n"," ('u.k.', 33),\n"," ('u.k/', 33),\n"," ('u.k', 33),\n"," ('s. africasouth africa', 26),\n"," ('s. africasouth africa/', 26)]"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["matches = fuzzywuzzy.process.extract(\"s.a\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","matches"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Replacement completed\n","Replacement completed\n","Replacement completed\n","Replacement completed\n","Replacement completed\n","Replacement completed\n","Replacement completed\n","Replacement completed\n"]}],"source":["def replace_country_names(df, column, string_to_match, min_ratio):\n","    \"\"\"\n","    Replaces country names that are a close match using fuzzy logic and a custom ratio\n","\n","    Arguments:  df - the dataframe to search\n","                column - the column to search\n","                string_to_match - the string to search for\n","                ratio - the minimum ratio to consider a match\n","    \"\"\"\n","\n","    # Get a list of unique names for the specified column\n","    unique_strings = df[column].unique()\n","\n","    # Get the top 10 closest matches for the string_to_match value\n","    matches = fuzzywuzzy.process.extract(string_to_match, unique_strings,\n","                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","    # Only get matches with a ratio greater than or equal to min_ratio value\n","    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n","\n","    # Get the rows of all the close matches in our dataframe\n","    rows_with_matches = df[column].isin(close_matches)\n","\n","    # Replace all rows with close matches with the input matches\n","    df.loc[rows_with_matches, column] = string_to_match\n","\n","     # Let us know when the function is done\n","    print(\"Replacement completed\")\n","\n","\n","# The min ratios were determined in the previous code cell by analyzing the relevant\n","# top 10 closest matches for each string_to_match value\n","replace_country_names(df=income_df, column='country', string_to_match=\"uk\", min_ratio=40)\n","replace_country_names(df=income_df, column='country', string_to_match=\"united kingdom\", min_ratio=90)\n","replace_country_names(df=income_df, column='country', string_to_match=\"britain\", min_ratio=90)\n","replace_country_names(df=income_df, column='country', string_to_match=\"england\", min_ratio=90)\n","replace_country_names(df=income_df, column='country', string_to_match=\"united states\", min_ratio=70)\n","replace_country_names(df=income_df, column='country', string_to_match=\"america\", min_ratio=90)\n","replace_country_names(df=income_df, column='country', string_to_match=\"south africa\", min_ratio=75)\n","replace_country_names(df=income_df, column='country', string_to_match=\"s.a.\", min_ratio=40)\n"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","Name replaced\n","\n","Number of unique countries: 3\n"]},{"data":{"text/plain":["array(['United Kingdom', 'South Africa', 'United States'], dtype=object)"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["def replace_name(string_to_find, new_name):\n","    \"\"\"\n","    Replaces the specified country name with a specified new name\n","\n","    Arguments:  string_to_find - country name to be replaced\n","                new_name - name to replace the old name with\n","    \"\"\"\n","    income_df.replace(string_to_find, new_name, inplace=True)\n","\n","    # Let us know when the function is done\n","    print(\"Name replaced\")\n","\n","\n","replace_name(string_to_find='united states', new_name=\"United States\")\n","replace_name(string_to_find='america', new_name=\"United States\")\n","replace_name(string_to_find='s.a.', new_name=\"South Africa\")\n","replace_name(string_to_find='britain', new_name=\"United Kingdom\")\n","replace_name(string_to_find='united kingdom', new_name=\"United Kingdom\")\n","replace_name(string_to_find='uk', new_name=\"United Kingdom\")\n","replace_name(string_to_find='england', new_name=\"United Kingdom\")\n","replace_name(string_to_find='/', new_name=\"unknown\")\n","replace_name(string_to_find='.', new_name=\"unknown\")\n","replace_name(string_to_find='', new_name=\"unknown\")\n","\n","# Display unique country names after all cleaning\n","countries = income_df['country'].unique()\n","print(f\"\\nNumber of unique countries: {len(countries)}\")\n","countries"]},{"cell_type":"markdown","metadata":{},"source":["3. Create a new column called `days_ago` in the DataFrame that is a copy of the 'date_measured' column but instead it is a number that shows how many days ago it was measured from the current date. Note that the current date can be obtained using `datetime.date.today()`."]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"data":{"text/plain":["0     4-2-2006\n","1     4-1-2006\n","2    12-9-2003\n","3     8-5-2006\n","4    21-1-1973\n","Name: date_measured, dtype: object"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["# modules we'll use\n","from datetime import date\n","# print the first few rows of the date column\n","income_df.date_measured.head()"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('O')"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["# check the data type of our date column\n","income_df['date_measured'].dtype"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"data":{"text/plain":["0   2006-02-04\n","1   2006-01-04\n","2   2003-09-12\n","3   2006-05-08\n","4   1973-01-21\n","Name: days_ago, dtype: datetime64[ns]"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["# create a new column, days_ago, with the parsed dates\n","income_df['date_measured'] = pd.to_datetime(income_df['date_measured'], format='%d-%m-%Y')\n","\n","income_df['days_ago'] = pd.to_datetime(income_df['date_measured'], format='%d %B %Y')\n"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Current date and time :  2024-04-24 13:21:37.504630\n"]}],"source":["# current date and time\n","from datetime import datetime\n","today = datetime.now()\n","print ('Current date and time : ', today)"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"data":{"text/plain":["0   2006-02-04\n","1   2006-01-04\n","2   2003-09-12\n","3   2006-05-08\n","4   1973-01-21\n","Name: days_ago, dtype: datetime64[ns]"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["#income_df.head(20)\n","income_df['days_ago'].head()"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["array(['United Kingdom', 'South Africa', 'United States'], dtype=object)"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["countries = income_df['country'].unique()\n","countries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 ('phd')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"vscode":{"interpreter":{"hash":"63d17dc58a06b6a6d4136fb13c245dafcf53668da37b1c3052c24d689135f5bb"}}},"nbformat":4,"nbformat_minor":0}
